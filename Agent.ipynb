{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2ba68c-c32f-4bbf-ab2d-bf537450ef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, display, Audio\n",
    "from ipywebrtc import CameraStream, AudioRecorder\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import torch\n",
    "import io\n",
    "\n",
    "model, utils = torch.hub.load(\n",
    "    repo_or_dir='snakers4/silero-vad',\n",
    "    model='silero_vad',\n",
    "    force_reload=False\n",
    ")\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n",
    "\n",
    "class AudioStream:\n",
    "    def __init__(self, chunk_ms=100):\n",
    "        self.stream = CameraStream(constraints={\n",
    "            'audio': {\n",
    "                'echoCancellation': True,\n",
    "                'noiseSuppression': True,\n",
    "                'sampleRate': 16000\n",
    "            },\n",
    "            'video': False\n",
    "        })\n",
    "        self.recorder = AudioRecorder(stream=self.stream, autosave=False)\n",
    "        self.recorder.audio.observe(self.on_audio_recorded, names='value')\n",
    "        self.audio_buffer = deque()\n",
    "        self.buffer_lock = threading.Lock()  # Thread safety\n",
    "        self.chunk_ms = chunk_ms\n",
    "        self.running = False\n",
    "        \n",
    "    def control(self):\n",
    "        display(self.stream)\n",
    "        display(self.recorder)\n",
    "    \n",
    "    def on_audio_recorded(self, change):\n",
    "        try:\n",
    "            audio_bytes = io.BytesIO(change['new']).getvalue()\n",
    "            if audio_bytes != b'':\n",
    "                audio = read_audio(audio_bytes, sampling_rate=16000)\n",
    "                with self.buffer_lock:  # Thread-safe append\n",
    "                    self.audio_buffer.append(audio)\n",
    "        except Exception as e:\n",
    "            print(f\"Error recording audio: {e}\")\n",
    "\n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        def record_loop():\n",
    "            while self.running:\n",
    "                self.recorder.recording = True\n",
    "                time.sleep(self.chunk_ms / 1000)\n",
    "                self.recorder.recording = False\n",
    "        self.thread = threading.Thread(target=record_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=1)\n",
    "\n",
    "    def show(self):\n",
    "        while True:\n",
    "            with self.buffer_lock:  # Thread-safe read\n",
    "                if len(self.audio_buffer) > 0:\n",
    "                    clear_output(wait=True)\n",
    "                    buffer = torch.cat(list(self.audio_buffer[-int(1000/self.chunk_ms * 5):]))\n",
    "                    plt.figure(figsize=(10, 3))\n",
    "                    plt.plot(buffer)\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "            time.sleep(0.1)\n",
    "\n",
    "stream = AudioStream(chunk_ms=500)\n",
    "stream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea0410-5c03-4e1c-af2e-846456fea330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n",
      "Abort all response and listen\n",
      "Generate Response now\n"
     ]
    }
   ],
   "source": [
    "class VoiceAgent:\n",
    "    def __init__(self, audio_buffer, buffer_lock, vad_thresh: float = 0.1):\n",
    "        self.audio_buffer = audio_buffer\n",
    "        self.buffer_lock = buffer_lock  # Store lock reference\n",
    "        self.is_responding = True\n",
    "        self.user_speaking = False\n",
    "        self.last_response = 0\n",
    "        self.logs = []\n",
    "        \n",
    "        # Use global model instead of reloading\n",
    "        self.vad = model\n",
    "        self.vad_thresh = vad_thresh\n",
    "\n",
    "    def update_state(self, conf):\n",
    "        if conf > self.vad_thresh:\n",
    "            if self.is_responding:\n",
    "                self.user_speaking = True\n",
    "                self.is_responding = False\n",
    "                print(f\"Abort all response and listen\")\n",
    "                self.last_response = len(self.logs)\n",
    "        else:\n",
    "            if self.user_speaking:\n",
    "                self.user_speaking = False\n",
    "                self.is_responding = True\n",
    "                print(f\"Generate Response now\")\n",
    "                # display(Audio(torch.cat(self.logs[self.last_response:-1]), rate=16000))\n",
    "\n",
    "    def infer_frame(self, frame):\n",
    "        assert len(frame) == 512\n",
    "        conf = self.vad(frame, 16000).item()\n",
    "        self.logs.append(frame)\n",
    "        self.update_state(conf)\n",
    "    \n",
    "    def run(self):\n",
    "        frame = torch.Tensor([])\n",
    "        \n",
    "        while True:\n",
    "            # Thread-safe buffer access\n",
    "            with self.buffer_lock:\n",
    "                if len(self.audio_buffer) == 0:\n",
    "                    chunk = None\n",
    "                else:\n",
    "                    chunk = self.audio_buffer.popleft()\n",
    "            \n",
    "            # Sleep if no data to avoid busy-waiting\n",
    "            if chunk is None:\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    "            \n",
    "            # Process any complete frames we already have\n",
    "            while len(frame) >= 512:\n",
    "                self.infer_frame(frame[:512])\n",
    "                frame = frame[512:]\n",
    "            \n",
    "            # Add new chunk data to frame\n",
    "            frame = torch.cat([frame, chunk]) if len(frame) > 0 else chunk\n",
    "            \n",
    "            # Process all complete frames from combined data\n",
    "            while len(frame) >= 512:\n",
    "                self.infer_frame(frame[:512])\n",
    "                frame = frame[512:]\n",
    "\n",
    "agent = VoiceAgent(stream.audio_buffer, stream.buffer_lock)\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8fc890-f854-42a8-ab84-1b2dfb46f37e",
   "metadata": {},
   "source": [
    "## Qwen3 Omni vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac34b3-f05a-4402-a837-3a36f0597f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 17:22:32 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved', 'interleaved'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section', 'interleaved'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 17:22:41 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 17:22:41 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-19 17:22:41 [config.py:1472] Using max model len 16384\n",
      "INFO 11-19 17:22:43 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=5120.\n",
      "INFO 11-19 17:22:43 [llm_engine.py:230] Initializing a V0 LLM engine (v0.11.1rc7.dev231+g8bd45fc0b.d20251119) with config: model='Qwen/Qwen3-Omni-30B-A3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen3-Omni-30B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=Qwen/Qwen3-Omni-30B-A3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":64,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 11-19 17:22:44 [cuda.py:363] Using Flash Attention backend.\n",
      "INFO 11-19 17:22:44 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-19 17:22:44 [model_runner.py:1171] Starting to load model Qwen/Qwen3-Omni-30B-A3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 17:22:45 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9cace39ecc424494cf3011eaa01af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 17:23:20 [default_loader.py:272] Loading weights took 34.85 seconds\n",
      "INFO 11-19 17:23:21 [model_runner.py:1203] Model loading took 59.1623 GiB and 35.545065 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 17:23:25 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 80) is too short to hold the multi-modal embeddings in the worst case (390 tokens in total, out of which {'audio': 390} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/vllm_qwen/vllm/model_executor/layers/rotary_embedding.py:1658: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor([1] * torch.tensor(video_grid_thw).shape[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 17:23:26 [fused_moe.py:690] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/vllm_qwen/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_A100_80GB_PCIe.json\n",
      "INFO 11-19 17:23:28 [worker.py:294] Memory profiling takes 7.12 seconds\n",
      "INFO 11-19 17:23:28 [worker.py:294] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.95) = 75.29GiB\n",
      "INFO 11-19 17:23:28 [worker.py:294] model weights take 59.16GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 3.91GiB; the rest of the memory reserved for KV Cache is 12.13GiB.\n",
      "INFO 11-19 17:23:28 [executor_base.py:113] # cuda blocks: 8278, # CPU blocks: 2730\n",
      "INFO 11-19 17:23:28 [executor_base.py:118] Maximum concurrency for 16384 tokens per request: 8.08x\n",
      "INFO 11-19 17:23:32 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265a14bd68fc46538f13f0144d97683f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 17:23:42 [model_runner.py:1671] Graph capturing finished in 10 secs, took 0.25 GiB\n",
      "INFO 11-19 17:23:42 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 20.95 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a852e469ca44fce9547326b2212bfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ User speaking - listening...\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, display, Audio\n",
    "from ipywebrtc import CameraStream, AudioRecorder\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import torch\n",
    "import io\n",
    "import os\n",
    "import tempfile\n",
    "import torchaudio\n",
    "import asyncio\n",
    "from ipywidgets import Output\n",
    "from queue import Queue\n",
    "\n",
    "# VAD Model\n",
    "model, utils = torch.hub.load(\n",
    "    repo_or_dir='snakers4/silero-vad',\n",
    "    model='silero_vad',\n",
    "    force_reload=False\n",
    ")\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n",
    "\n",
    "# LLM Setup\n",
    "os.environ['VLLM_USE_V1'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "from transformers import Qwen3OmniMoeProcessor\n",
    "\n",
    "MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n",
    "# MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "\n",
    "# Create AsyncLLMEngine\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    limit_mm_per_prompt={'image': 0, 'video': 0, 'audio': 1},\n",
    "    max_num_seqs=64,\n",
    "    max_model_len=16384,\n",
    "    dtype=torch.float16,\n",
    "    enable_chunked_prefill=True\n",
    ")\n",
    "\n",
    "llm = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "class AudioStream:\n",
    "    def __init__(self, chunk_ms=100):\n",
    "        self.stream = CameraStream(constraints={\n",
    "            'audio': {\n",
    "                'echoCancellation': True,\n",
    "                'noiseSuppression': True,\n",
    "                'sampleRate': 16000\n",
    "            },\n",
    "            'video': False\n",
    "        })\n",
    "        self.recorder = AudioRecorder(stream=self.stream, autosave=False)\n",
    "        self.recorder.audio.observe(self.on_audio_recorded, names='value')\n",
    "        self.audio_buffer = deque()\n",
    "        self.buffer_lock = threading.Lock()  # Thread safety\n",
    "        self.chunk_ms = chunk_ms\n",
    "        self.running = False\n",
    "        \n",
    "    def control(self):\n",
    "        display(self.stream)\n",
    "        display(self.recorder)\n",
    "    \n",
    "    def on_audio_recorded(self, change):\n",
    "        try:\n",
    "            audio_data = change['new']\n",
    "            if audio_data and len(audio_data) > 0:\n",
    "                # Pass BytesIO object directly to read_audio, not raw bytes\n",
    "                audio_io = io.BytesIO(audio_data)\n",
    "                audio = read_audio(audio_io, sampling_rate=16000)\n",
    "                with self.buffer_lock:  # Thread-safe append\n",
    "                    self.audio_buffer.append(audio)\n",
    "        except Exception as e:\n",
    "            # Suppress common empty audio errors\n",
    "            if \"End of file\" not in str(e) and \"incompatible function arguments\" not in str(e):\n",
    "                print(f\"Error recording audio: {e}\")\n",
    "\n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        def record_loop():\n",
    "            while self.running:\n",
    "                self.recorder.recording = True\n",
    "                time.sleep(self.chunk_ms / 1000)\n",
    "                self.recorder.recording = False\n",
    "        self.thread = threading.Thread(target=record_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=1)\n",
    "\n",
    "    def show(self):\n",
    "        while True:\n",
    "            with self.buffer_lock:  # Thread-safe read\n",
    "                if len(self.audio_buffer) > 0:\n",
    "                    clear_output(wait=True)\n",
    "                    buffer = torch.cat(list(self.audio_buffer[-int(1000/self.chunk_ms * 5):]))\n",
    "                    plt.figure(figsize=(10, 3))\n",
    "                    plt.plot(buffer)\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "            time.sleep(0.1)\n",
    "\n",
    "stream = AudioStream(chunk_ms=500)\n",
    "stream.start()\n",
    "\n",
    "class VoiceAgent:\n",
    "    def __init__(self, audio_buffer, buffer_lock, llm, processor, vad_thresh: float = 0.1):\n",
    "        self.audio_buffer = audio_buffer\n",
    "        self.buffer_lock = buffer_lock  # Store lock reference\n",
    "        self.is_responding = True\n",
    "        self.user_speaking = False\n",
    "        self.last_response = 0\n",
    "        self.logs = []\n",
    "        self.generating = False  # Track if LLM is generating\n",
    "        \n",
    "        # Use global model instead of reloading\n",
    "        self.vad = model\n",
    "        self.vad_thresh = vad_thresh\n",
    "        \n",
    "        # LLM components\n",
    "        self.llm = llm\n",
    "        self.processor = processor\n",
    "        \n",
    "        # Create event loop for async operations\n",
    "        self.loop = None\n",
    "        \n",
    "        # Output widget for Jupyter display\n",
    "        self.output_widget = Output()\n",
    "        display(self.output_widget)\n",
    "        \n",
    "        # Queue for streaming tokens\n",
    "        self.token_queue = Queue()\n",
    "\n",
    "    def update_state(self, conf):\n",
    "        if conf > self.vad_thresh:\n",
    "            if self.is_responding:\n",
    "                self.user_speaking = True\n",
    "                self.is_responding = False\n",
    "                with self.output_widget:\n",
    "                    print(f\"ðŸŽ¤ User speaking - listening...\")\n",
    "                self.last_response = len(self.logs)\n",
    "        else:\n",
    "            if self.user_speaking and not self.generating:\n",
    "                self.user_speaking = False\n",
    "                self.is_responding = True\n",
    "                with self.output_widget:\n",
    "                    print(f\"ðŸ¤– Generating response...\")\n",
    "                # Run generation in background thread with async\n",
    "                threading.Thread(target=self._run_async_generation, daemon=True).start()\n",
    "    \n",
    "    def _run_async_generation(self):\n",
    "        \"\"\"Run async generation in a new event loop\"\"\"\n",
    "        # Create new event loop for this thread\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        try:\n",
    "            loop.run_until_complete(self._generate_response())\n",
    "        finally:\n",
    "            loop.close()\n",
    "    \n",
    "    async def _generate_response(self):\n",
    "        \"\"\"Generate LLM response from user audio with streaming\"\"\"\n",
    "        try:\n",
    "            self.generating = True\n",
    "            \n",
    "            with self.output_widget:\n",
    "                print(\"[DEBUG] Starting generation...\")\n",
    "            \n",
    "            # Get user audio from logs\n",
    "            if self.last_response >= len(self.logs):\n",
    "                with self.output_widget:\n",
    "                    print(\"No audio to process\")\n",
    "                return\n",
    "            \n",
    "            user_audio = torch.cat(self.logs[self.last_response:])\n",
    "            \n",
    "            with self.output_widget:\n",
    "                print(f\"[DEBUG] User audio shape: {user_audio.shape}\")\n",
    "            \n",
    "            # Save audio to temporary file for processing\n",
    "            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:\n",
    "                tmp_path = tmp_file.name\n",
    "                torchaudio.save(tmp_path, user_audio.unsqueeze(0), 16000)\n",
    "            \n",
    "            with self.output_widget:\n",
    "                print(f\"[DEBUG] Audio saved to: {tmp_path}\")\n",
    "            \n",
    "            # Prepare messages\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Respond to this audio naturally and concisely\"},\n",
    "                        {\"type\": \"audio\", \"audio\": tmp_path},\n",
    "                    ], \n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Process with vLLM\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                top_k=20,\n",
    "                max_tokens=512,\n",
    "            )\n",
    "            \n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            \n",
    "            inputs = {\n",
    "                'prompt': text,\n",
    "                'multi_modal_data': {\n",
    "                    \"audio\": [user_audio]\n",
    "                },\n",
    "                \"mm_processor_kwargs\": {\n",
    "                    \"use_audio_in_video\": True,\n",
    "                },\n",
    "            }\n",
    "            \n",
    "            # Generate unique request ID\n",
    "            request_id = f\"request-{time.time()}\"\n",
    "            \n",
    "            # Stream outputs asynchronously\n",
    "            with self.output_widget:\n",
    "                print(\"[DEBUG] Starting stream generation...\")\n",
    "                print(\"Assistant: \", end=\"\", flush=True)\n",
    "            \n",
    "            full_response = \"\"\n",
    "            \n",
    "            # Use async generator to stream tokens - AsyncLLMEngine uses different API\n",
    "            with self.output_widget:\n",
    "                print(\"[DEBUG] Calling llm.generate()...\")\n",
    "            \n",
    "            results_generator = self.llm.generate(\n",
    "                prompt=text,\n",
    "                sampling_params=sampling_params,\n",
    "                request_id=request_id,\n",
    "                multi_modal_data={\n",
    "                    \"audio\": [user_audio]\n",
    "                },\n",
    "                mm_processor_kwargs={\n",
    "                    \"use_audio_in_video\": True,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            with self.output_widget:\n",
    "                print(\"[DEBUG] Got generator, starting iteration...\")\n",
    "            \n",
    "            iteration_count = 0\n",
    "            async for request_output in results_generator:\n",
    "                iteration_count += 1\n",
    "                with self.output_widget:\n",
    "                    print(f\"[DEBUG] Iteration {iteration_count}, outputs: {len(request_output.outputs) if request_output.outputs else 0}\")\n",
    "                \n",
    "                # RequestOutput has outputs list with CompletionOutput objects\n",
    "                if request_output.outputs:\n",
    "                    # Get the generated text so far\n",
    "                    current_text = request_output.outputs[0].text\n",
    "                    \n",
    "                    # Print only the new tokens (delta)\n",
    "                    new_text = current_text[len(full_response):]\n",
    "                    if new_text:\n",
    "                        # Display in output widget for Jupyter\n",
    "                        with self.output_widget:\n",
    "                            print(new_text, end=\"\", flush=True)\n",
    "                        full_response = current_text\n",
    "            \n",
    "            with self.output_widget:\n",
    "                print(f\"[DEBUG] Loop completed after {iteration_count} iterations\")\n",
    "            \n",
    "            # New line after completion\n",
    "            with self.output_widget:\n",
    "                print()  # New line\n",
    "                print(f\"[Complete] Total tokens: {len(full_response.split())}\")\n",
    "            \n",
    "            # Cleanup temp file\n",
    "            os.unlink(tmp_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self.output_widget:\n",
    "                print(f\"\\n[ERROR] Error generating response: {e}\")\n",
    "                import traceback\n",
    "                print(\"[ERROR] Full traceback:\")\n",
    "                traceback.print_exc()\n",
    "        finally:\n",
    "            with self.output_widget:\n",
    "                print(\"[DEBUG] Setting generating=False\")\n",
    "            self.generating = False\n",
    "\n",
    "    def infer_frame(self, frame):\n",
    "        assert len(frame) == 512\n",
    "        conf = self.vad(frame, 16000).item()\n",
    "        self.logs.append(frame)\n",
    "        self.update_state(conf)\n",
    "    \n",
    "    def run(self):\n",
    "        frame = torch.Tensor([])\n",
    "        \n",
    "        while True:\n",
    "            # Thread-safe buffer access\n",
    "            with self.buffer_lock:\n",
    "                if len(self.audio_buffer) == 0:\n",
    "                    chunk = None\n",
    "                else:\n",
    "                    chunk = self.audio_buffer.popleft()\n",
    "            \n",
    "            # Sleep if no data to avoid busy-waiting\n",
    "            if chunk is None:\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    "            \n",
    "            # Process any complete frames we already have\n",
    "            while len(frame) >= 512:\n",
    "                self.infer_frame(frame[:512])\n",
    "                frame = frame[512:]\n",
    "            \n",
    "            # Add new chunk data to frame\n",
    "            frame = torch.cat([frame, chunk]) if len(frame) > 0 else chunk\n",
    "            \n",
    "            # Process all complete frames from combined data\n",
    "            while len(frame) >= 512:\n",
    "                self.infer_frame(frame[:512])\n",
    "                frame = frame[512:]\n",
    "\n",
    "agent = VoiceAgent(stream.audio_buffer, stream.buffer_lock, llm, processor)\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc2d91-bf82-4fe7-a348-c63608b64864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv(qwen_omni)",
   "language": "python",
   "name": "vllm_qwen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
